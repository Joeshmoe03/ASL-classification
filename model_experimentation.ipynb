{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0 - Introduction and Install Dependencies** (_if needed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U argparse\n",
    "#%pip install -U tqdm\n",
    "#%pip install -U scikit-learn\n",
    "#%pip install -U opencv-python\n",
    "#%pip install -U matplotlib\n",
    "#%pip install -U tensorflow\n",
    "#%pip install -U pandas\n",
    "#%pip install -U numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from tensorflow.keras import Input, Model # type: ignore\n",
    "from tensorflow.keras.layers import AveragePooling2D, Flatten, Dense, Dropout, Add # type: ignore\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "import os\n",
    "import argparse\n",
    "from tensorflow.keras.models import Sequential, Model # type: ignore\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Data Loading Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLDataPaths():\n",
    "    '''\n",
    "    fetchASLDataPaths is a class that fetches the paths of the ASL dataset from a directory. The rationale behind such a class\n",
    "    is the fact that our dataset is huge (relatively speaking), and we can not afford to load the entire dataset of images into memory.\n",
    "    Rather, it might be a better idea to load the paths of the images, and then load the images in batches as we train our model. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, data_dir: str):\n",
    "\n",
    "        # Check if the data directory exists\n",
    "        if type(data_dir) != str or not os.path.exists(data_dir):\n",
    "            raise FileNotFoundError(f\"The directory {data_dir} does not exist.\")\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def fetch_paths(self):\n",
    "        X_paths = []\n",
    "        y = []\n",
    "\n",
    "        # Walk over the data directory and fetch the paths of all images, label in the dataset\n",
    "        for root, _, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                X_paths.append(os.path.join(root, file))\n",
    "                y.append(os.path.basename(root))\n",
    "        \n",
    "        X_paths = np.array(X_paths)\n",
    "        y = np.array(y)\n",
    "        return X_paths, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ASLBatchLoader class is a custom data loader following the concept of this documentation code: \n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset. Refer to this documentation for more information\n",
    "# and context on how to implement a custom data loader in TensorFlow.\n",
    "class ASLBatchLoader(tf.keras.utils.PyDataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 X_set: np.array, \n",
    "                 y_set: np.array,\n",
    "                 batch_size: int = 32, \n",
    "                 transform = None):\n",
    "        '''\n",
    "        The ASLBatchLoader class is a custom data loader that loads the ASL dataset in batches.\n",
    "        \n",
    "        Parameters:\n",
    "            X_set: np.array - A numpy array containing the paths of the images and \n",
    "            y_set: np.array - their corresponding labels.\n",
    "            batch_size: int - The size of the batch that we want to load the data in.\n",
    "        '''\n",
    "        self.X_set = X_set\n",
    "        self.y_set = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28}\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        This function returns the number of batches that we can load from the dataset.\n",
    "        \n",
    "        Returns: int - The number of batches that we can load from the dataset.\n",
    "        '''\n",
    "        return math.ceil(len(self.X_set) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        This function loads a batch of data from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            index: int - The index of batch that we want to load from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            X_batch: np.array - A numpy array containing the images of the batch.\n",
    "            y_batch: np.array - A numpy array containing labels of the batch.\n",
    "        '''\n",
    "        # We specify the start of our batch\n",
    "        batch_start = index * self.batch_size\n",
    "\n",
    "        # If the batch end is greater than the length of the data directory, we set the batch end to the length of the data directory\n",
    "        batch_end = min(batch_start + self.batch_size, len(self.X_set))\n",
    "\n",
    "        # These are the paths that we immediately work with in this iteration of the batching process\n",
    "        X_path_batch = self.X_set[batch_start:batch_end]\n",
    "        y_batch = self.y_set[batch_start:batch_end]\n",
    "\n",
    "        # We convert the labels to their corresponding indices\n",
    "        y_batch = np.array([self.mapping[label] for label in y_batch])\n",
    "        y_batch_encoded = tf.one_hot(y_batch, 29)\n",
    "\n",
    "        # Load the images and labels from the paths\n",
    "        # If a transformation is specified, we apply it to the images\n",
    "        # If no transformation is specified, we simply load the images\n",
    "        # A transformation is typically something like normalization, resizing, etc.\n",
    "        X_batch = np.array([cv2.imread(file) for file in X_path_batch])\n",
    "        if self.transform is not None:\n",
    "            X_batch = self.transform(X_batch)\n",
    "\n",
    "        return X_batch, y_batch_encoded\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        This method returns an iterator for the batches.\n",
    "        \n",
    "        Yields: batch - A batch of data from the dataset.\n",
    "        '''\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT was used to generate these docstring. No need to do redundant work.\n",
    "def split_data(data, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    '''\n",
    "    Split the data into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "        data: contains X, y as np.array\n",
    "        test_size: float - The size of the test set.\n",
    "        val_size: float - The size of the validation set.\n",
    "        random_state: int - The random state for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: a tuple of np.arrays - train_data, val_data, test_data\n",
    "    '''\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=val_size, random_state=random_state)\n",
    "    return (train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 - Tansforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert an image to grayscale. Used by the transform pipeline in Lambda layer.\n",
    "def grayscale(img):\n",
    "    return tf.image.rgb_to_grayscale(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = './data/asl_alphabet_train/asl_alphabet_train/'\n",
    "\n",
    "# Hyperparameters\n",
    "batchSize = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "momentum = 0.8\n",
    "optimizer = 'SGD'\n",
    "loss = 'sparse_categorical_crossentropy' # categorical_crossentropy\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "from_logits = False\n",
    "early_stopping = None\n",
    "val_split = 0.2\n",
    "test_split = 0.2\n",
    "model = 'resnet' # vgg\n",
    "\n",
    "#TODO: implement\n",
    "pretrain = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - Our Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimizerFactory(optimizer: str, learning_rate: float, momentum: float, weight_decay: float):\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer: {optimizer}\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossFactory(loss: str, from_logits: bool):\n",
    "    if loss == 'categorical_crossentropy':\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=from_logits)\n",
    "    elif loss == 'sparse_categorical_crossentropy':\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=from_logits)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid loss function: {loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelFactory(model: str):\n",
    "    if model == 'resnet':\n",
    "        model = resnet\n",
    "    elif model == 'vgg':\n",
    "        NotImplementedError(\"VGG not implemented yet...\") # TODO: NOT IMPLEMENTED YET\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model: {model}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Preprocess Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up our transform pipeline. Feel free to modify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: FEEL FREE TO MODIFY TRANSFORMATIONS AS NEEDED\n",
    "transform = tf.keras.Sequential([\n",
    "                                layers.Resizing(224, 224),\n",
    "                                layers.Rescaling(1./255),\n",
    "                                layers.RandomFlip(\"horizontal\"),\n",
    "                                layers.RandomRotation(0.2),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data paths\n",
    "X_path, y = ASLDataPaths(data_dir = data_dir).fetch_paths()\n",
    "X_train_path, X_test_path, y_train, y_test = train_test_split(X_path, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train_path, X_val_path, y_train, y_val = train_test_split(X_train_path, y_train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Load the data into our batchloader\n",
    "train_batch_loader = ASLBatchLoader(X_set = X_train_path, y_set = y_train, batch_size=batchSize, transform = transform)\n",
    "val_batch_loader = ASLBatchLoader(X_set = X_val_path, y_set = y_val, batch_size=batchSize, transform = transform)\n",
    "test_batch_loader = ASLBatchLoader(X_set = X_test_path, y_set = y_test, batch_size=batchSize, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch our optimizer, loss and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = OptimizerFactory(optimizer, learning_rate, momentum, weight_decay)\n",
    "loss = LossFactory(loss, from_logits)\n",
    "model = ModelFactory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 29])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
    "#checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = os.path.join(os.getcwd(), 'resnet.model.h5'), save_best_only = True, verbose = 1)\n",
    "#if early_stopping is not None:\n",
    "#    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = early_stopping, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_batch_loader, validation_data = val_batch_loader, epochs = epochs, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
