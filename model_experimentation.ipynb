{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0 - Introduction and Install Dependencies** (_if needed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are seeing this notebook for the first time and would like to run it, please follow these steps. Note that you will need several (>9) GB of free space in order to succesfully do the following:\n",
    "\n",
    "1) In the same directory where this notebook is currently located on your device, make a new folder called \"data\\\".\n",
    "2) Visit this [page on Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet) and download the dataset.\n",
    "3) Extract the data from the compressed file into the \"data\\\" folder, such that your current directory mirrors the following (thank you for helping me generate this figure ChatGPT):\n",
    "\n",
    "```\n",
    "root_directory\n",
    "   |- exploratory_analysis.ipynb\n",
    "   |- data\n",
    "      |- asl_alphabet_test\n",
    "         |- data...\n",
    "      |- asl_alphabet_train\n",
    "         |- data...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U argparse\n",
    "#%pip install -U tqdm\n",
    "#%pip install -U scikit-learn\n",
    "#%pip install -U opencv-python\n",
    "#%pip install -U matplotlib\n",
    "#%pip install -U tensorflow\n",
    "#%pip install -U pandas\n",
    "#%pip install -U numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten # type: ignore\n",
    "from keras.models import Sequential # type: ignore\n",
    "import tensorflow as tf # type: ignore\n",
    "import os\n",
    "import argparse\n",
    "from keras.models import Sequential, Model # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from numpy.random import seed # type: ignore\n",
    "import cv2 # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import math\n",
    "import random\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for tf.data.Dataset inspired by: https://stackoverflow.com/questions/58270150/is-there-some-simple-way-to-apply-image-preprocess-to-tf-data-dataset\n",
    "def transformTrainData(image, label):\n",
    "    '''\n",
    "    See the documentation for tf.image for more information on the transformations: \n",
    "    https://www.tensorflow.org/api_docs/python/tf/image\n",
    "    '''\n",
    "    # Performs scaling. NOTE: FEEL FREE TO MODIFY. SEE DOCUMENTATION ABOVE TO FIND MORE TRANSFORMATIONS.\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.rot90(image, k = random.randint(0, 3))\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    return image, label\n",
    "\n",
    "def transformValData(image, label):\n",
    "    '''\n",
    "    See the documentation for tf.image for more information on the transformations:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/image\n",
    "    '''\n",
    "    # Performs scaling\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 - Hyperparamters** (_feel free to adjust_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a splitting for our train, val, and test data, and other hyperparameters\n",
    "val_split = 0.2\n",
    "# Image size\n",
    "img_size = 64\n",
    "# Batch size. This is the number of images that will be fed into the model at once. Adjust based on your memory capacity.\n",
    "batch_size = 32\n",
    "# Color mode\n",
    "color = 'rgb' # 'grayscale'\n",
    "# Seed for reproducibility\n",
    "seed = 42\n",
    "# Number of epochs\n",
    "epochs = 10\n",
    "# Loss function\n",
    "loss = 'categorical_crossentropy'#'sparse_categorical_crossentropy' # 'categorical_crossentropy' (one-hot encoded labels) or 'sparse_categorical_crossentropy' (integer labels\n",
    "# Optimizer\n",
    "optimizer = 'adam' # 'sgd'\n",
    "# Regularization parameters\n",
    "wd = None\n",
    "# momentum is a hyperparameter for the optimizer SGD that helps accelerate the convergence of the model\n",
    "momentum = 9e-06\n",
    "# learning rate is a hyperparameter that controls how much we are adjusting the weights of our network with respect the loss gradient\n",
    "lr = 0.001\n",
    "# Apply softmax to the output layer?\n",
    "from_logits = False\n",
    "# Metrics to track\n",
    "metrics = ['accuracy'] # 'accuracy', 'f1_score', 'precision', 'recall'\n",
    "# number of classes\n",
    "num_classes = 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data directories\n",
    "data_path = \"./data/asl_alphabet_train/asl_alphabet_train/\"\n",
    "test_path = \"./data/asl_alphabet_test/asl_alphabet_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a tf.data.Dataset\n",
    "train_dataset, val_dataset = tf.keras.utils.image_dataset_from_directory(data_path, \n",
    "                                                                         labels = 'inferred', \n",
    "                                                                         label_mode = 'int' if loss == 'sparse_categorical_crossentropy' else 'categorical', \n",
    "                                                                         color_mode = color, \n",
    "                                                                         batch_size = batch_size, \n",
    "                                                                         image_size = (img_size, img_size), \n",
    "                                                                         shuffle = True, \n",
    "                                                                         seed = seed, \n",
    "                                                                         validation_split = val_split, \n",
    "                                                                         subset = \"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the transformations to the dataset\n",
    "train_dataset = train_dataset.map(transformTrainData)\n",
    "val_dataset = val_dataset.map(transformValData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet50 model\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False, #True\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(img_size, img_size, 3 if color == 'rgb' else 1),\n",
    "    pooling=None,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)  \n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax')(x)  \n",
    "resnet = tf.keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection of the model\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 - Other utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizerFactory(optim: str, lr: float, momentum: float, wd: float):\n",
    "    '''\n",
    "    Choose an optimizer based on the passed in string.\n",
    "    '''\n",
    "    if optim == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate = lr, momentum = momentum, decay = wd)\n",
    "    elif optim == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Optimizer {optim} not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFactory(loss: str, from_logits: bool = False):\n",
    "    if loss == 'sparse_categorical_crossentropy':\n",
    "        return tf.keras.losses.SparseCategoricalCrossentropy(from_logits = from_logits)\n",
    "    elif loss == 'categorical_crossentropy':\n",
    "        return tf.keras.losses.CategoricalCrossentropy(from_logits = from_logits)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Loss {loss} not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6 - Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=['accuracy']#, tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.F1Score(average='macro'),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify our optimizer and loss function, and compile the model with the metrics we want to track\n",
    "optimizer = optimizerFactory(optimizer, lr, momentum, wd)\n",
    "loss = lossFactory(loss, from_logits)\n",
    "model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = metrics) #optimizer, #loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RUN THIS NEXT CELL AT YOUR OWN RISK. PLEASE USE A GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data = val_dataset, epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of submission, we are still in the process of training. We will be running a script version of this jupyter notebook on a HPC overnight. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
