{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U homeharvest\n",
    "%pip install -U tqdm\n",
    "%pip install -U scikit-learn\n",
    "%pip install -U opencv-python\n",
    "%pip install -U matplotlib\n",
    "%pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1 - Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn\n",
    "from cv2 import imread\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: you can adjust these following parameters to generate/load data corresponding to them.** You have several options:\n",
    "\n",
    "1) If you have already downloaded the raw data and extracted it to the /raw_data/ folder, you can regenerate data with new coloring/sizing options, but it may take a few minutes... Adjusting the parameters in the next cell will make those changes during regeneration. \n",
    "\n",
    "2) Otherwise, you can skip running any cells under section 2 and rely on the saved data formatted as a numpy array under /rgbZ/ or /grayscaleZ/ under /data/ to work with, where Z represents the image dimensions. Again, you can adjust the parameters in the next cell depending on which saved data you want to work with (assuming it exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale = True\n",
    "img_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 - Generate Data** \n",
    "**NOTE: only run this to generate newly formatted data (for different color scheme or size)**. Otherwise this will be very slow for you. There already exists a saved numpy array of the data, so you can skip running cells under section 2. Otherwise consider batch loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory where the data is stored. Training and testing, respectively.\n",
    "train_data = './raw_data/asl_alphabet_train/asl_alphabet_train/'\n",
    "test_data = './raw_data/asl_alphabet_test/asl_alphabet_test/'\n",
    "\n",
    "# Load the training data: see https://www.kaggle.com/code/shampasinha/interpret-sign-language-with-deep-learning \n",
    "# which was used as a reference for this function.\n",
    "# The dataset is collosal, so we will use batch processing to load the data by default.\n",
    "def load_train_data(data_dir, img_size=50, grayscale=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Our data is structured in terms of folders. We iterate through each folder and read the images. \n",
    "    # Each folder represents a letter in the alphabet. \n",
    "    for folder in os.listdir(data_dir):\n",
    "        for image in tqdm(os.listdir(data_dir + folder), desc=folder):\n",
    "\n",
    "            # Read the image\n",
    "            img = imread(data_dir + folder + '/' + image)\n",
    "            \n",
    "            # Convert the image to grayscale if needed\n",
    "            if grayscale:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Append the image and its corresponding label\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            images.append(img)\n",
    "            labels.append(str(folder))\n",
    "\n",
    "            # Skip images that are not read correctly\n",
    "            if img is None:\n",
    "                continue\n",
    "                \n",
    "    # Convert the lists to numpy arrays. It is more efficient to work with numpy arrays...\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Normalize the images\n",
    "    X = X.astype('float32') / 255.0\n",
    "    return X, y\n",
    "\n",
    "X, y = load_train_data(train_data, img_size=img_size, grayscale=grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(X, y, grayscale=False, img_size=64):\n",
    "    if grayscale:\n",
    "        if not os.path.exists(f'./data/grayscale{img_size}'):\n",
    "            os.makedirs(f'./data/grayscale{img_size}')\n",
    "        X = np.save(f'./data/grayscale{img_size}/X.npy', X)\n",
    "        y = np.save(f'./data/grayscale{img_size}/y.npy', y)\n",
    "    else:\n",
    "        if not os.path.exists(f'./data/rgb{img_size}'):\n",
    "            os.makedirs(f'./data/rgb{img_size}')\n",
    "        X = np.save(f'./data/rgb{img_size}/X.npy', X)\n",
    "        y = np.save(f'./data/rgb{img_size}/y.npy', y)\n",
    "\n",
    "save_data(X, y, grayscale=grayscale, img_size=img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 - Quick-Load, Batching, and Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_load_data(grayscale=False, img_size=64):\n",
    "    if grayscale:\n",
    "        X = np.load(f'./data/grayscale{img_size}/X.npy')\n",
    "        y = np.load(f'./data/grayscale{img_size}/y.npy')\n",
    "    else:\n",
    "        X = np.load(f'./data/rgb{img_size}/X.npy')\n",
    "        y = np.load(f'./data/rgb{img_size}/y.npy')\n",
    "    return X, y\n",
    "\n",
    "X, y = quick_load_data(grayscale=grayscale, img_size=img_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # We shuffle later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use batch sizes in the context of working with large datasets more efficiently.\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataset from the numpy arrays. Tensorflow allows us to use batching.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "# We shuffle the dataset and batch it. Buffer size exists for large datasets - especially ones that can't all fit memory.\n",
    "# We shuffle also to prevent any biases in the data - each batch will be more representative of the data. Batching is used\n",
    "# to make life easy and memory not overloaded. We can't possibly load all the data into memory at once.\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_data = val_data.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4 - Inspect and Analyze**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets inspect that the image is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grayscale:\n",
    "    color = 'gray'\n",
    "else:\n",
    "    color = 'viridis'\n",
    "\n",
    "# Plot the images and labels\n",
    "random.seed(42)\n",
    "indices = random.choices(range(len(X_train)), k=9)\n",
    "n_row = 3\n",
    "n_col = 3\n",
    "fig, axes = plt.subplots(n_row, n_col, figsize=(13, 13))\n",
    "for i in range(n_row):\n",
    "    for j in range(n_col):\n",
    "        axes[i, j].imshow(X_train[indices[i * n_col + j]], cmap=color)\n",
    "        axes[i, j].set_title(f\"Label: {y_train[indices[i * n_col + j]]}\")\n",
    "        axes[i, j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the distribution of labels appears uniform (something that makes clear sense if you inspect raw_data). Relatively balanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check how balanced the dataset is\n",
    "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "plt.title('Label Distribution')\n",
    "plt.bar(unique_labels, label_counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5 - First-Attempt Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, labels in train_data:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
